import dotenv from "dotenv";
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StrOutputParser } from "@langchain/core/output_parsers";
import { Tool } from "@langchain/core/tools";
import { z } from "zod";
import { AgentExecutor, createToolCallingAgent } from "langchain/agents";
import { MessagesPlaceholder } from "@langchain/core/messages";
import { FakeEmbeddings } from "@langchain/community/embeddings/fake";
import { Chroma } from "@langchain/community/vectorstores/chroma";

dotenv.config({ override: true });

const apiKey = process.env.OPENAI_API_KEY;
const baseURL = process.env.OPENAI_BASE_URL || "https://api.openai.com/v1";
const modelName = process.env.MODEL_NAME || "gpt-3.5-turbo";
const langchainApiKey = process.env.LANGCHAIN_API_KEY;

if (!apiKey) {
  console.error("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡");
  process.exit(1);
}

interface PerformanceMetrics {
  chainName: string;
  executionTime: number;
  inputTokens: number;
  outputTokens: number;
  totalTokens: number;
  success: boolean;
  errorMessage: string;
}

class ProductionMonitor {
  metricsHistory: PerformanceMetrics[] = [];
  startTime: number | null = null;

  startTracking(): void {
    this.startTime = Date.now();
  }

  endTracking(
    chainName: string,
    success: boolean,
    error: string = ""
  ): PerformanceMetrics {
    if (!this.startTime) {
      throw new Error("å¿…é¡»å…ˆè°ƒç”¨ startTracking()");
    }

    const executionTime = (Date.now() - this.startTime) / 1000;

    const metrics: PerformanceMetrics = {
      chainName,
      executionTime,
      inputTokens: 0,
      outputTokens: 0,
      totalTokens: 0,
      success,
      errorMessage: error,
    };

    this.metricsHistory.push(metrics);
    this.startTime = null;

    return metrics;
  }

  getSummary(): Record<string, unknown> {
    if (this.metricsHistory.length === 0) {
      return { message: "æ²¡æœ‰è®°å½•çš„æŒ‡æ ‡" };
    }

    const totalRuns = this.metricsHistory.length;
    const successfulRuns = this.metricsHistory.filter((m) => m.success).length;
    const failedRuns = totalRuns - successfulRuns;

    const avgTime =
      this.metricsHistory.reduce((sum, m) => sum + m.executionTime, 0) / totalRuns;
    const totalTokens = this.metricsHistory.reduce((sum, m) => sum + m.totalTokens, 0);

    return {
      totalRuns,
      successfulRuns,
      failedRuns,
      successRate: successfulRuns / totalRuns,
      averageTime: avgTime,
      totalTokens,
      estimatedCost: totalTokens * 0.00002,
    };
  }

  saveMetrics(filename: string = "performance_metrics.json"): void {
    const data = {
      timestamp: new Date().toISOString(),
      summary: this.getSummary(),
      metrics: this.metricsHistory,
    };

    const fs = require("fs");
    fs.writeFileSync(filename, JSON.stringify(data, null, 2), "utf-8");
    console.log(`âœ“ æŒ‡æ ‡å·²ä¿å­˜åˆ° ${filename}`);
  }
}

class CustomLogger {
  logs: string[] = [];

  log(level: string, message: string): void {
    const timestamp = new Date().toLocaleString("zh-CN");
    const logEntry = `[${timestamp}] [${level}] ${message}`;
    this.logs.push(logEntry);
    console.log(logEntry);
  }

  saveLogs(filename: string = "execution_logs.txt"): void {
    const fs = require("fs");
    fs.writeFileSync(filename, this.logs.join("\n"), "utf-8");
    console.log(`âœ“ æ—¥å¿—å·²ä¿å­˜åˆ° ${filename}`);
  }
}

function setupLangsmith(): boolean {
  if (!langchainApiKey) {
    console.log("âš ï¸  æœªè®¾ç½® LANGCHAIN_API_KEYï¼ŒLangSmith è¿½è¸ªå·²ç¦ç”¨");
    console.log("   è®¿é—® https://smith.langchain.com/ è·å– API Key");
    return false;
  }

  process.env.LANGCHAIN_TRACING_V2 = "true";
  process.env.LANGCHAIN_API_KEY = langchainApiKey;
  console.log("âœ“ LangSmith è¿½è¸ªå·²å¯ç”¨");
  return true;
}

async function example1SimpleChainWithTracing(
  monitor: ProductionMonitor,
  logger: CustomLogger
): Promise<void> {
  console.log("\n" + "=".repeat(60));
  console.log("ç¤ºä¾‹ 1: ç®€å• Chain è¿½è¸ª");
  console.log("=".repeat(60));

  try {
    const llm = new ChatOpenAI({
      modelName,
      openAIApiKey: apiKey,
      configuration: { baseURL },
      temperature: 0,
    });

    const prompt = ChatPromptTemplate.fromTemplate("å›ç­”ï¼š{question}");

    const chain = prompt.pipe(llm).pipe(new StrOutputParser());

    monitor.startTracking();

    const response = await chain.invoke(
      { question: "ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ" },
      {
        tags: ["production", "simple"],
        metadata: { version: "1.0", user_id: "demo" },
      }
    );

    const metrics = monitor.endTracking("simple_chain", true);
    console.log(`å“åº”: ${response}`);
    console.log(`æ‰§è¡Œæ—¶é—´: ${metrics.executionTime.toFixed(2)}ç§’`);
  } catch (e) {
    const metrics = monitor.endTracking("simple_chain", false, e instanceof Error ? e.message : String(e));
    console.log(`é”™è¯¯: ${e}`);
    logger.log("ERROR", `ç®€å• Chain é”™è¯¯: ${e}`);
  }
}

async function example2AgentWithTracing(
  monitor: ProductionMonitor,
  logger: CustomLogger
): Promise<void> {
  console.log("\n" + "=".repeat(60));
  console.log("ç¤ºä¾‹ 2: Agent è¿½è¸ª");
  console.log("=".repeat(60));

  try {
    const llm = new ChatOpenAI({
      modelName,
      openAIApiKey: apiKey,
      configuration: { baseURL },
      temperature: 0,
    });

    const calculator = new Tool({
      name: "calculator",
      description: "è®¡ç®—æ•°å­¦è¡¨è¾¾å¼",
      schema: z.object({
        expression: z.string().describe("æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ 25 * 4"),
      }),
      func: async (input: { expression: string }) => {
        try {
          const result = eval(input.expression);
          return `è®¡ç®—ç»“æœ: ${result}`;
        } catch {
          return "è®¡ç®—é”™è¯¯";
        }
      },
    });

    const tools = [calculator];

    const prompt = ChatPromptTemplate.fromMessages([
      ["system", "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨è®¡ç®—å™¨å·¥å…·ã€‚"],
      ["user", "{input}"],
      new MessagesPlaceholder("agent_scratchpad"),
    ]);

    const agent = await createToolCallingAgent({ llm, tools, prompt });
    const agentExecutor = new AgentExecutor({
      agent,
      tools,
      verbose: true,
      maxIterations: 3,
    });

    monitor.startTracking();

    const response = await agentExecutor.invoke(
      { input: "è®¡ç®— 25 * 4 + 18 ç­‰äºå¤šå°‘ï¼Ÿ" },
      {
        tags: ["production", "agent"],
        metadata: { version: "1.0", agent_type: "calculator" },
      }
    );

    const metrics = monitor.endTracking("agent_chain", true);
    console.log(`å“åº”: ${response.output}`);
    console.log(`æ‰§è¡Œæ—¶é—´: ${metrics.executionTime.toFixed(2)}ç§’`);
  } catch (e) {
    const metrics = monitor.endTracking("agent_chain", false, e instanceof Error ? e.message : String(e));
    console.log(`é”™è¯¯: ${e}`);
    logger.log("ERROR", `Agent é”™è¯¯: ${e}`);
  }
}

async function example3RAGWithTracing(
  monitor: ProductionMonitor,
  logger: CustomLogger
): Promise<void> {
  console.log("\n" + "=".repeat(60));
  console.log("ç¤ºä¾‹ 3: RAG è¿½è¸ª");
  console.log("=".repeat(60));

  try {
    const llm = new ChatOpenAI({
      modelName,
      openAIApiKey: apiKey,
      configuration: { baseURL },
      temperature: 0,
    });

    const documents = [
      "LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ã€‚",
      "LangChain æä¾›äº†é“¾å¼è°ƒç”¨ã€æç¤ºè¯ç®¡ç†ç­‰åŠŸèƒ½ã€‚",
      "LangChain æ”¯æŒå¤šç§ LLM æä¾›å•†å’Œå·¥å…·ã€‚",
    ];

    const embeddings = new FakeEmbeddings({ size: 1536 });
    const vectorstore = await Chroma.fromTexts(documents, [], embeddings);
    const retriever = vectorstore.asRetriever();

    const prompt = ChatPromptTemplate.fromTemplate(`
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š
`);

    const chain = (
      {
        context: retriever,
        question: (x: { question: string }) => x.question,
      } as any
    )
      .pipe(prompt)
      .pipe(llm)
      .pipe(new StrOutputParser());

    monitor.startTracking();

    const response = await chain.invoke(
      { question: "LangChain æœ‰ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ" },
      {
        tags: ["production", "rag"],
        metadata: { version: "1.0", retriever_type: "chroma" },
      }
    );

    const metrics = monitor.endTracking("rag_chain", true);
    console.log(`å“åº”: ${response}`);
    console.log(`æ‰§è¡Œæ—¶é—´: ${metrics.executionTime.toFixed(2)}ç§’`);
  } catch (e) {
    const metrics = monitor.endTracking("rag_chain", false, e instanceof Error ? e.message : String(e));
    console.log(`é”™è¯¯: ${e}`);
    logger.log("ERROR", `RAG é”™è¯¯: ${e}`);
  }
}

async function example4PerformanceComparison(
  monitor: ProductionMonitor,
  logger: CustomLogger
): Promise<void> {
  console.log("\n" + "=".repeat(60));
  console.log("ç¤ºä¾‹ 4: æ€§èƒ½å¯¹æ¯”");
  console.log("=".repeat(60));

  try {
    const testQuestion = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ";

    console.log(`\næµ‹è¯•æ¨¡å‹: ${modelName}`);

    const llm = new ChatOpenAI({
      modelName,
      openAIApiKey: apiKey,
      configuration: { baseURL },
      temperature: 0,
    });

    monitor.startTracking();

    const response = await llm.invoke(testQuestion);

    const metrics = monitor.endTracking(`model_${modelName}`, true);
    console.log(`å“åº”é•¿åº¦: ${response.content.toString().length} å­—ç¬¦`);
    console.log(`æ‰§è¡Œæ—¶é—´: ${metrics.executionTime.toFixed(2)}ç§’`);
  } catch (e) {
    const metrics = monitor.endTracking(
      `model_${modelName}`,
      false,
      e instanceof Error ? e.message : String(e)
    );
    console.log(`é”™è¯¯: ${e}`);
    logger.log("ERROR", `æ€§èƒ½å¯¹æ¯”é”™è¯¯: ${e}`);
  }
}

async function main() {
  console.log("ğŸ¦œğŸ”— 11 - ç”Ÿäº§çº§è¿½è¸ª");
  console.log("=".repeat(60));

  const langsmithEnabled = setupLangsmith();

  const monitor = new ProductionMonitor();
  const logger = new CustomLogger();

  try {
    await example1SimpleChainWithTracing(monitor, logger);
    await example2AgentWithTracing(monitor, logger);
    await example3RAGWithTracing(monitor, logger);
    await example4PerformanceComparison(monitor, logger);

    console.log("\n" + "=".repeat(60));
    console.log("æ€§èƒ½æ‘˜è¦");
    console.log("=".repeat(60));

    const summary = monitor.getSummary();
    console.log(JSON.stringify(summary, null, 2));

    monitor.saveMetrics();
    logger.saveLogs();

    if (langsmithEnabled) {
      console.log("\nâœ“ è®¿é—® LangSmith æŸ¥çœ‹è¯¦ç»†è¿½è¸ª:");
      console.log("  https://smith.langchain.com/");
    }

    console.log("\nğŸ‰ ç”Ÿäº§çº§è¿½è¸ªç¤ºä¾‹è¿è¡Œå®Œæˆï¼");
  } catch (e) {
    console.log(`âŒ è¿è¡Œé”™è¯¯ï¼š${e}`);
    process.exit(1);
  }
}

main().catch(console.error);
