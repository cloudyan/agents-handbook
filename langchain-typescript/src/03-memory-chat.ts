import dotenv from "dotenv";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { BaseMessage, HumanMessage, AIMessage } from "@langchain/core/messages";
import { createModelClient } from "./clients/model";

// åŠ è½½ç¯å¢ƒå˜é‡ï¼Œè¦†ç›–å·²å­˜åœ¨çš„å˜é‡
dotenv.config({ override: true });

class BufferMemory {
  private messages: BaseMessage[] = [];
  private maxMessages: number;

  constructor(maxMessages: number = 10) {
    this.maxMessages = maxMessages;
  }

  addMessage(message: BaseMessage): void {
    this.messages.push(message);
    if (this.messages.length > this.maxMessages) {
      this.messages = this.messages.slice(-this.maxMessages);
    }
  }

  getMessages(): BaseMessage[] {
    return this.messages;
  }

  clear(): void {
    this.messages = [];
  }
}

// ğŸ’¬ å¸¦è®°å¿†çš„å¯¹è¯
async function memoryChat() {
  const model = createModelClient();

  const memory = new BufferMemory(5);

  const prompt = ChatPromptTemplate.fromMessages([
    ["system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ï¼Œä¼šè®°ä½ä¹‹å‰çš„å¯¹è¯å†…å®¹ã€‚"],
    ["placeholder", "{chat_history}"],
    ["human", "{input}"],
  ]);

  const chain = prompt.pipe(model);

  async function chat(userInput: string): Promise<void> {
    console.log(`\nä½ : ${userInput}`);

    const messages = memory.getMessages();
    const result = await chain.invoke({
      input: userInput,
      chat_history: messages,
    });

    const response = result.content as string;
    console.log(`åŠ©æ‰‹: ${response}`);

    memory.addMessage(new HumanMessage(userInput));
    memory.addMessage(new AIMessage(response));
  }

  await chat("æˆ‘å«å°æ˜");
  await chat("æˆ‘ä»Šå¹´25å²");
  await chat("æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ");
  await chat("æˆ‘å¤šå¤§äº†ï¼Ÿ");

  console.log("\n" + "=".repeat(50));
  console.log("å¯¹è¯å®Œæˆï¼åŠ©æ‰‹è®°ä½äº†ä½ çš„ä¿¡æ¯ã€‚");
}

memoryChat().catch(console.error);
