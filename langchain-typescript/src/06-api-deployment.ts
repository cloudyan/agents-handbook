import dotenv from "dotenv";
import express from "express";
import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage } from "@langchain/core/messages";

// åŠ è½½ç¯å¢ƒå˜é‡ï¼Œè¦†ç›–å·²å­˜åœ¨çš„å˜é‡
dotenv.config({ override: true });

const app = express();
const PORT = process.env.PORT || 3000;

app.use(express.json());

const apiKey = process.env.OPENAI_API_KEY;
const baseURL = process.env.OPENAI_BASE_URL || "https://api.openai.com/v1";
const modelName = process.env.MODEL_NAME || "gpt-3.5-turbo";

if (!apiKey) {
  console.error("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡");
  process.exit(1);
}

const llm = new ChatOpenAI({
  modelName,
  openAIApiKey: apiKey,
  configuration: { baseURL },
  temperature: 0.7,
});

app.get("/", (req, res) => {
  res.json({
    message: "LangChain TypeScript API Server",
    endpoints: {
      "/chat": "POST - å‘é€èŠå¤©æ¶ˆæ¯",
      "/health": "GET - å¥åº·æ£€æŸ¥",
    },
  });
});

app.get("/health", (req, res) => {
  res.json({ status: "ok", timestamp: new Date().toISOString() });
});

app.post("/chat", async (req, res) => {
  try {
    const { message } = req.body;

    if (!message) {
      return res.status(400).json({ error: "è¯·æä¾› message å‚æ•°" });
    }

    console.log(`ğŸ“¤ æ”¶åˆ°æ¶ˆæ¯: ${message}`);

    const response = await llm.invoke([new HumanMessage(message)]);

    console.log(`ğŸ“¥ å›å¤: ${response.content}`);

    res.json({
      message: response.content,
      timestamp: new Date().toISOString(),
    });
  } catch (error) {
    console.error("å¤„ç†è¯·æ±‚æ—¶å‡ºé”™:", error);
    res.status(500).json({ error: "å¤„ç†è¯·æ±‚å¤±è´¥" });
  }
});

app.listen(PORT, () => {
  console.log("ğŸš€ LangChain TypeScript API Server");
  console.log("=".repeat(50));
  console.log(`ğŸ“¡ æœåŠ¡å™¨è¿è¡Œåœ¨ http://localhost:${PORT}`);
  console.log(`ğŸ“š API æ–‡æ¡£: http://localhost:${PORT}/`);
  console.log("=".repeat(50));
});
