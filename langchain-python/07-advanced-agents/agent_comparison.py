#!/usr/bin/env python3
"""
Agentæ€§èƒ½å¯¹æ¯”åˆ†æå·¥å…·
æ¯”è¾ƒä¸åŒAgentç±»å‹çš„æ€§èƒ½è¡¨ç°
"""

import time
import json
import statistics
from typing import Dict, List, Any
from dataclasses import dataclass
from dotenv import load_dotenv
from pydantic import SecretStr
import os

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(override=True)


@dataclass
class AgentMetrics:
    """Agentæ€§èƒ½æŒ‡æ ‡"""

    name: str
    response_time: float
    success_rate: float
    token_usage: int
    tool_calls: int
    reasoning_steps: int
    accuracy_score: float


class AgentComparator:
    """Agentæ€§èƒ½å¯¹æ¯”å™¨"""

    def __init__(self):
        self.test_questions = [
            {
                "question": "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
                "expected_keywords": ["ç¼–ç¨‹è¯­è¨€", "é«˜çº§è¯­è¨€", "è§£é‡Šå‹"],
                "difficulty": "ç®€å•",
            },
            {
                "question": "è®¡ç®— 15 * 8 + 32 ç­‰äºå¤šå°‘ï¼Ÿ",
                "expected_answer": "152",
                "difficulty": "ç®€å•",
            },
            {
                "question": "LangChainçš„ä¸»è¦åŠŸèƒ½æœ‰å“ªäº›ï¼Ÿè¯·è¯¦ç»†è¯´æ˜ã€‚",
                "expected_keywords": ["LLM", "æç¤ºè¯", "é“¾", "æ™ºèƒ½ä½“"],
                "difficulty": "ä¸­ç­‰",
            },
            {
                "question": "åˆ†ææœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åŒºåˆ«ï¼Œå¹¶ç»™å‡ºåº”ç”¨åœºæ™¯ã€‚",
                "expected_keywords": ["ç¥ç»ç½‘ç»œ", "æ•°æ®é‡", "å¤æ‚åº¦", "åº”ç”¨"],
                "difficulty": "å›°éš¾",
            },
        ]

        self.results = []

    def evaluate_response(self, question: Dict, response: str) -> float:
        """è¯„ä¼°å›ç­”å‡†ç¡®æ€§"""
        score = 0.0

        # æ£€æŸ¥æœŸæœ›å…³é”®è¯
        if "expected_keywords" in question:
            keyword_count = sum(
                1
                for keyword in question["expected_keywords"]
                if keyword.lower() in response.lower()
            )
            score += (keyword_count / len(question["expected_keywords"])) * 0.6

        # æ£€æŸ¥æœŸæœ›ç­”æ¡ˆ
        if "expected_answer" in question:
            if question["expected_answer"] in response:
                score += 0.8

        # æ£€æŸ¥å›ç­”è´¨é‡
        if len(response) > 50:
            score += 0.2  # å†…å®¹å……è¶³
        if "ã€‚" in response or "." in response:
            score += 0.1  # æœ‰å®Œæ•´å¥å­

        return min(score, 1.0)

    def test_agent(self, agent, agent_name: str) -> AgentMetrics:
        """æµ‹è¯•å•ä¸ªAgent"""
        print(f"\n=== æµ‹è¯• {agent_name} ===")

        response_times = []
        success_count = 0
        token_usages = []
        tool_calls_list = []
        reasoning_steps_list = []
        accuracy_scores = []

        for i, question in enumerate(self.test_questions):
            print(f"é—®é¢˜ {i + 1}: {question['question']}")

            try:
                start_time = time.time()
                response = agent.invoke({"input": question["question"]})
                end_time = time.time()

                response_time = end_time - start_time
                response_times.append(response_time)

                # è¯„ä¼°å›ç­”
                answer = response.get("output", "")
                accuracy = self.evaluate_response(question, answer)
                accuracy_scores.append(accuracy)

                success_count += 1

                # æ¨¡æ‹Ÿå…¶ä»–æŒ‡æ ‡ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦ä»Agentæ‰§è¡Œä¸­è·å–ï¼‰
                token_usages.append(len(answer.split()) * 2)  # ä¼°ç®—tokenä½¿ç”¨
                tool_calls_list.append(2)  # å‡è®¾å¹³å‡2æ¬¡å·¥å…·è°ƒç”¨
                reasoning_steps_list.append(3)  # å‡è®¾å¹³å‡3ä¸ªæ¨ç†æ­¥éª¤

                print(f"å›ç­”ï¼š{answer[:100]}...")
                print(f"å‡†ç¡®æ€§ï¼š{accuracy:.2f}")
                print(f"å“åº”æ—¶é—´ï¼š{response_time:.2f}ç§’")

            except Exception as e:
                print(f"é”™è¯¯ï¼š{e}")
                response_times.append(0)
                accuracy_scores.append(0)
                token_usages.append(0)
                tool_calls_list.append(0)
                reasoning_steps_list.append(0)

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        metrics = AgentMetrics(
            name=agent_name,
            response_time=statistics.mean(response_times),
            success_rate=success_count / len(self.test_questions),
            token_usage=statistics.mean(token_usages),
            tool_calls=statistics.mean(tool_calls_list),
            reasoning_steps=statistics.mean(reasoning_steps_list),
            accuracy_score=statistics.mean(accuracy_scores),
        )

        self.results.append(metrics)
        return metrics

    def generate_comparison_report(self) -> str:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        if not self.results:
            return "æ²¡æœ‰æµ‹è¯•ç»“æœ"

        report = []
        report.append("=" * 60)
        report.append("Agentæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        report.append("=" * 60)
        report.append("")

        # è¯¦ç»†æŒ‡æ ‡è¡¨æ ¼
        report.append("è¯¦ç»†æ€§èƒ½æŒ‡æ ‡ï¼š")
        report.append("-" * 60)
        report.append(
            f"{'Agentåç§°':<15} {'å“åº”æ—¶é—´(s)':<10} {'æˆåŠŸç‡':<8} {'Tokenæ•°':<8} {'å·¥å…·è°ƒç”¨':<8} {'æ¨ç†æ­¥éª¤':<8} {'å‡†ç¡®æ€§':<8}"
        )
        report.append("-" * 60)

        for metrics in self.results:
            report.append(
                f"{metrics.name:<15} "
                f"{metrics.response_time:<10.2f} "
                f"{metrics.success_rate:<8.2%} "
                f"{metrics.token_usage:<8.0f} "
                f"{metrics.tool_calls:<8.1f} "
                f"{metrics.reasoning_steps:<8.1f} "
                f"{metrics.accuracy_score:<8.2%}"
            )

        report.append("")

        # æ€§èƒ½æ’å
        report.append("æ€§èƒ½æ’åï¼š")
        report.append("-" * 30)

        rankings = {
            "å“åº”é€Ÿåº¦": sorted(self.results, key=lambda x: x.response_time),
            "å‡†ç¡®æ€§": sorted(
                self.results, key=lambda x: x.accuracy_score, reverse=True
            ),
            "æˆåŠŸç‡": sorted(self.results, key=lambda x: x.success_rate, reverse=True),
            "æ•ˆç‡": sorted(
                self.results, key=lambda x: x.token_usage / (x.accuracy_score + 0.01)
            ),
        }

        for metric_name, ranking in rankings.items():
            report.append(f"\n{metric_name}æ’åï¼š")
            for i, metrics in enumerate(ranking, 1):
                report.append(f"  {i}. {metrics.name}")

        # æ¨èå»ºè®®
        report.append("\n" + "=" * 30)
        report.append("æ¨èå»ºè®®ï¼š")
        report.append("-" * 30)

        best_accuracy = max(self.results, key=lambda x: x.accuracy_score)
        fastest = min(self.results, key=lambda x: x.response_time)
        most_reliable = max(self.results, key=lambda x: x.success_rate)

        report.append(
            f"â€¢ æœ€é«˜å‡†ç¡®æ€§ï¼š{best_accuracy.name} (å‡†ç¡®æ€§: {best_accuracy.accuracy_score:.2%})"
        )
        report.append(
            f"â€¢ æœ€å¿«å“åº”ï¼š{fastest.name} (å“åº”æ—¶é—´: {fastest.response_time:.2f}s)"
        )
        report.append(
            f"â€¢ æœ€å¯é ï¼š{most_reliable.name} (æˆåŠŸç‡: {most_reliable.success_rate:.2%})"
        )

        # ä½¿ç”¨åœºæ™¯å»ºè®®
        report.append("\nä½¿ç”¨åœºæ™¯å»ºè®®ï¼š")
        report.append("-" * 20)

        for metrics in self.results:
            if metrics.accuracy_score > 0.8:
                scenario = "é«˜è´¨é‡é—®ç­”ã€çŸ¥è¯†æ£€ç´¢"
            elif metrics.response_time < 2.0:
                scenario = "å®æ—¶å¯¹è¯ã€å¿«é€Ÿå“åº”"
            elif metrics.success_rate > 0.9:
                scenario = "ç”Ÿäº§ç¯å¢ƒã€å…³é”®ä»»åŠ¡"
            else:
                scenario = "å¼€å‘æµ‹è¯•ã€å®éªŒæ€§åº”ç”¨"

            report.append(f"â€¢ {metrics.name}: {scenario}")

        return "\n".join(report)

    def save_results(self, filename: str = "agent_comparison_results.json"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        data = {
            "timestamp": time.time(),
            "test_questions": self.test_questions,
            "results": [
                {
                    "name": m.name,
                    "response_time": m.response_time,
                    "success_rate": m.success_rate,
                    "token_usage": m.token_usage,
                    "tool_calls": m.tool_calls,
                    "reasoning_steps": m.reasoning_steps,
                    "accuracy_score": m.accuracy_score,
                }
                for m in self.results
            ],
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ç»“æœå·²ä¿å­˜åˆ° {filename}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ Agentæ€§èƒ½å¯¹æ¯”åˆ†æå·¥å…·")
    print("=" * 50)

    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    openai_api_key = os.getenv("OPENAI_API_KEY", "")
    openai_base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    model_name = os.getenv("MODEL_NAME", "gpt-3.5-turbo")

    # æ£€æŸ¥ç¯å¢ƒ
    if not openai_api_key:
        print("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return 1

    try:
        # å¯¼å…¥LangChainç»„ä»¶
        from langchain_openai import ChatOpenAI
        from langchain.agents import (
            tool,
            AgentExecutor,
            create_react_agent,
            create_tool_calling_agent,
        )
        from langchain_core.prompts import (
            PromptTemplate,
            ChatPromptTemplate,
            MessagesPlaceholder,
        )

        print("âœ“ ç»„ä»¶å¯¼å…¥æˆåŠŸ")

        # åˆå§‹åŒ–LLM
        llm = ChatOpenAI(
            model=model_name,
            temperature=0,
            api_key=SecretStr(openai_api_key),
            base_url=openai_base_url
        )

        # åˆ›å»ºæµ‹è¯•å·¥å…·
        @tool
        def search_info(query: str) -> str:
            """æœç´¢ä¿¡æ¯å·¥å…·"""
            database = {
                "Python": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumåˆ›å»ºï¼Œå…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½ã€‚",
                "LangChain": "LangChainæ˜¯ç”¨äºæ„å»ºLLMåº”ç”¨çš„æ¡†æ¶ï¼Œæä¾›äº†é“¾å¼è°ƒç”¨ã€æç¤ºè¯ç®¡ç†ã€æ™ºèƒ½ä½“ç­‰åŠŸèƒ½ã€‚",
                "æœºå™¨å­¦ä¹ ": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„åˆ†æ”¯ï¼Œè®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ã€‚",
                "æ·±åº¦å­¦ä¹ ": "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘ï¼Œæ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸã€‚",
            }

            for key, value in database.items():
                if query.lower() in key.lower():
                    return value

            return f"æœªæ‰¾åˆ°å…³äº'{query}'çš„ä¿¡æ¯"

        @tool
        def calculate(expression: str) -> str:
            """æ•°å­¦è®¡ç®—å·¥å…·"""
            try:
                # å®‰å…¨çš„æ•°å­¦è¡¨è¾¾å¼è®¡ç®—
                allowed_chars = set("0123456789+-*/().")
                if all(c in allowed_chars for c in expression):
                    result = eval(expression)
                    return f"è®¡ç®—ç»“æœï¼š{result}"
                else:
                    return "è¡¨è¾¾å¼åŒ…å«ä¸å…è®¸çš„å­—ç¬¦"
            except:
                return "è®¡ç®—é”™è¯¯ï¼Œè¯·æ£€æŸ¥è¡¨è¾¾å¼"

        tools = [search_info, calculate]

        # åˆ›å»ºä¸åŒç±»å‹çš„Agent
        agents = {}

        # 1. ReAct Agent
        react_prompt = PromptTemplate.from_template("""
        å›ç­”é—®é¢˜ï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™äº›å·¥å…·ï¼š
        {tools}

        ä½¿ç”¨æ ¼å¼ï¼š
        Question: é—®é¢˜
        Thought: æ€è€ƒ
        Action: å·¥å…·
        Action Input: è¾“å…¥
        Observation: ç»“æœ
        ... (é‡å¤)
        Final Answer: æœ€ç»ˆç­”æ¡ˆ

        Question: {input}
        Thought: {agent_scratchpad}
        """)

        react_agent = create_react_agent(llm, tools, react_prompt)
        agents["ReAct"] = AgentExecutor(agent=react_agent, tools=tools, verbose=False)

        # 2. Tool Calling Agent
        functions_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·è·å–ä¿¡æ¯å¹¶è¿›è¡Œè®¡ç®—ã€‚"),
                ("user", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad"),
            ]
        )

        functions_agent = create_tool_calling_agent(llm, tools, functions_prompt)
        agents["Tool Calling"] = AgentExecutor(
            agent=functions_agent, tools=tools, verbose=False
        )

        # 3. ç®€å•Chainï¼ˆä½œä¸ºå¯¹æ¯”åŸºå‡†ï¼‰
        simple_prompt = PromptTemplate.from_template("è¯·å›ç­”ï¼š{input}")
        simple_chain = simple_prompt | llm

        class SimpleAgentWrapper:
            def __init__(self, chain):
                self.chain = chain

            def invoke(self, input_data):
                result = self.chain.invoke(input_data["input"])
                return {"output": result.content}

        agents["Simple Chain"] = SimpleAgentWrapper(simple_chain)

        # è¿è¡Œå¯¹æ¯”æµ‹è¯•
        comparator = AgentComparator()

        for agent_name, agent in agents.items():
            comparator.test_agent(agent, agent_name)

        # ç”ŸæˆæŠ¥å‘Š
        report = comparator.generate_comparison_report()
        print("\n" + report)

        # ä¿å­˜ç»“æœ
        comparator.save_results()

        print("\nğŸ‰ æ€§èƒ½å¯¹æ¯”åˆ†æå®Œæˆï¼")

    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯ï¼š{e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
