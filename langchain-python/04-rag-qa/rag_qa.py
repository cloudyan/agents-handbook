#!/usr/bin/env python3
"""
04 - RAG QA (LCEL ç‰ˆæœ¬)
å­¦ä¹ æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œé€šè¿‡æ–‡æ¡£æ£€ç´¢æ¥æé«˜é—®ç­”çš„å‡†ç¡®æ€§

å‚è€ƒ TypeScript ç‰ˆæœ¬å®ç°ï¼Œä½¿ç”¨ Ollama åšåµŒå…¥ï¼ŒChroma åšå‘é‡å­˜å‚¨
"""

import os
import sys
from dotenv import load_dotenv

load_dotenv(override=True)


def main():
    print("ğŸ¦œğŸ”— 04 - RAG QA (LCEL)")
    print("=" * 40)

    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return 1

    try:
        import requests
        from bs4 import BeautifulSoup
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        from langchain_chroma import Chroma
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.runnables import RunnablePassthrough

        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        from clients import create_model_client, create_embedding_client

        print("âœ“ LangChain ç»„ä»¶å¯¼å…¥å®Œæˆ")

        print("\n=== 1. å‡†å¤‡æ–‡æ¡£æ•°æ® ===")

        url = "https://docs.langchain.com/oss/python/langchain/overview"
        print(f"æ­£åœ¨è·å–æ–‡æ¡£: {url}")

        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            print(f"âœ“ æˆåŠŸè·å–æ–‡æ¡£ (çŠ¶æ€ç : {response.status_code})")

            soup = BeautifulSoup(response.content, 'html.parser')

            body_text = soup.body.get_text(separator='\n', strip=True)

            print(f"âœ“ æ–‡æ¡£è§£æå®Œæˆ")
            print(f"æ–‡æ¡£é•¿åº¦: {len(body_text)} å­—ç¬¦")

        except requests.RequestException as e:
            print(f"âš ï¸  è·å–æ–‡æ¡£å¤±è´¥: {e}")
            print("ä½¿ç”¨å¤‡ç”¨æ–‡æ¡£å†…å®¹...")

            body_text = """
            LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚
            å®ƒæä¾›äº†ä¸€å¥—å·¥å…·å’Œç»„ä»¶ï¼Œå¸®åŠ©å¼€å‘è€…æ›´å®¹æ˜“åœ°åˆ›å»ºå¤æ‚çš„ AI åº”ç”¨ã€‚

            LangChain çš„æ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š
            - æ¨¡å‹æŠ½è±¡ï¼šç»Ÿä¸€ä¸åŒ LLM æä¾›å•†çš„æ¥å£
            - æç¤ºè¯ç®¡ç†ï¼šåˆ›å»ºå’Œç®¡ç†å¤æ‚çš„æç¤ºè¯æ¨¡æ¿
            - é“¾å¼è°ƒç”¨ï¼šå°†å¤šä¸ªç»„ä»¶ä¸²è”æˆå·¥ä½œæµ
            - è®°å¿†ç®¡ç†ï¼šä¸ºå¯¹è¯ç³»ç»Ÿæ·»åŠ è®°å¿†åŠŸèƒ½
            - æ™ºèƒ½ä½“ï¼šåˆ›å»ºèƒ½å¤Ÿä½¿ç”¨å·¥å…·çš„è‡ªä¸»æ™ºèƒ½ä½“
            - ç´¢å¼•å’Œæ£€ç´¢ï¼šæ„å»º RAG ç³»ç»Ÿ

            LangChain æ”¯æŒå¤šç§ LLM æä¾›å•†ï¼ŒåŒ…æ‹¬ OpenAIã€Anthropicã€Hugging Face ç­‰ã€‚
            å®ƒè¿˜æä¾›äº†ä¸°å¯Œçš„é›†æˆï¼Œå¦‚å‘é‡æ•°æ®åº“ã€æ–‡æ¡£åŠ è½½å™¨ã€å·¥å…·ç­‰ã€‚
            """

        print("\n=== 2. åˆ†å‰²æ–‡æ¡£ ===")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
        )

        chunks = text_splitter.split_text(body_text)
        print(f"âœ“ æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} ä¸ªç‰‡æ®µ")

        print("\n=== 3. åˆ›å»ºå‘é‡ç´¢å¼• ===")

        print("ä½¿ç”¨ Ollama åµŒå…¥æ¨¡å‹...")
        embeddings = create_embedding_client(use_ollama=True)

        print("è¿æ¥åˆ° Chroma æœåŠ¡ (Docker)...")
        vector_store = Chroma.from_texts(
            texts=chunks,
            embedding=embeddings,
            metadatas=[{"source": "langchain-docs", "index": i} for i in range(len(chunks))],
            collection_name="rag-qa-demo",
            persist_directory=None,
        )
        print("âœ“ å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆ")

        print("\n=== 4. åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ ===")

        llm = create_model_client(temperature=0)

        prompt = ChatPromptTemplate.from_template("""
è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•å›ç­”ã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {input}

å›ç­”:
""")

        retriever = vector_store.as_retriever(search_kwargs={"k": 3})

        def format_docs(docs):
            """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
            return "\n\n".join(doc.page_content for doc in docs)

        rag_chain = (
            {"context": retriever | format_docs, "input": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        print("âœ“ RAG é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

        print("\n=== 5. æµ‹è¯•é—®ç­” ===")

        test_questions = [
            "å…³äº LangChain ä½ çŸ¥é“ä»€ä¹ˆï¼Ÿ",
            "LangChain æä¾›å“ªäº›æ ¸å¿ƒåŠŸèƒ½ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        ]

        for question in test_questions:
            print(f"\né—®é¢˜: {question}")
            print("-" * 50)

            result = rag_chain.invoke(question)

            print(f"å›ç­”: {result}")

        print("\n" + "=" * 50)
        print("RAG é—®ç­”ç³»ç»Ÿè¿è¡Œå®Œæˆï¼")

    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯ï¼š{e}")
        print("\nè¯·ç¡®ä¿å®‰è£…äº†ä»¥ä¸‹ä¾èµ–ï¼š")
        print("  pip install requests beautifulsoup4 langchain-text-splitters langchain-chroma langchain-ollama")
        return 1
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
