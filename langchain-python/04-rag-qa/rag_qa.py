#!/usr/bin/env python3
"""
04 - RAG QA (LCEL ç‰ˆæœ¬)
å­¦ä¹ æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œé€šè¿‡æ–‡æ¡£æ£€ç´¢æ¥æé«˜é—®ç­”çš„å‡†ç¡®æ€§
"""

import os
from dotenv import load_dotenv

load_dotenv(override=True)


def main():
    print("ğŸ¦œğŸ”— 04 - RAG QA (LCEL)")
    print("=" * 40)

    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return 1

    try:
        from langchain_openai import ChatOpenAI, OpenAIEmbeddings
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        from langchain_chroma import Chroma
        from langchain_community.document_loaders import TextLoader
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.runnables import RunnablePassthrough

        print("âœ“ LangChain ç»„ä»¶å¯¼å…¥å®Œæˆ")

        print("\n=== 1. å‡†å¤‡æ–‡æ¡£æ•°æ® ===")

        os.makedirs("temp_docs", exist_ok=True)

        sample_docs = [
            (
                "temp_docs/python_intro.txt",
                """
Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”± Guido van Rossum äº 1991 å¹´é¦–æ¬¡å‘å¸ƒã€‚
Python å…·æœ‰ç®€æ´æ˜äº†çš„è¯­æ³•ï¼Œæ˜“äºå­¦ä¹ å’Œä½¿ç”¨ï¼Œè¢«å¹¿æ³›åº”ç”¨äº Web å¼€å‘ã€
æ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½ã€è‡ªåŠ¨åŒ–è„šæœ¬ç­‰é¢†åŸŸã€‚

Python çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š
- è¯­æ³•ç®€æ´ï¼Œå¯è¯»æ€§å¼º
- æ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ï¼ˆé¢å‘å¯¹è±¡ã€å‡½æ•°å¼ã€è¿‡ç¨‹å¼ï¼‰
- ä¸°å¯Œçš„æ ‡å‡†åº“å’Œç¬¬ä¸‰æ–¹åº“
- è·¨å¹³å°ï¼Œå¯åœ¨å¤šç§æ“ä½œç³»ç»Ÿä¸Šè¿è¡Œ
- æ´»è·ƒçš„ç¤¾åŒºæ”¯æŒ
""",
            ),
            (
                "temp_docs/langchain_intro.txt",
                """
LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚
å®ƒæä¾›äº†ä¸€å¥—å·¥å…·å’Œç»„ä»¶ï¼Œå¸®åŠ©å¼€å‘è€…æ›´å®¹æ˜“åœ°åˆ›å»ºå¤æ‚çš„ AI åº”ç”¨ã€‚

LangChain çš„æ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š
- æ¨¡å‹æŠ½è±¡ï¼šç»Ÿä¸€ä¸åŒ LLM æä¾›å•†çš„æ¥å£
- æç¤ºè¯ç®¡ç†ï¼šåˆ›å»ºå’Œç®¡ç†å¤æ‚çš„æç¤ºè¯æ¨¡æ¿
- é“¾å¼è°ƒç”¨ï¼šå°†å¤šä¸ªç»„ä»¶ä¸²è”æˆå·¥ä½œæµ
- è®°å¿†ç®¡ç†ï¼šä¸ºå¯¹è¯ç³»ç»Ÿæ·»åŠ è®°å¿†åŠŸèƒ½
- æ™ºèƒ½ä½“ï¼šåˆ›å»ºèƒ½å¤Ÿä½¿ç”¨å·¥å…·çš„è‡ªä¸»æ™ºèƒ½ä½“
- ç´¢å¼•å’Œæ£€ç´¢ï¼šæ„å»º RAG ç³»ç»Ÿ
""",
            ),
        ]

        for file_path, content in sample_docs:
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content.strip())

        print("âœ“ ç¤ºä¾‹æ–‡æ¡£åˆ›å»ºå®Œæˆ")

        print("\n=== 2. åŠ è½½å’Œåˆ†å‰²æ–‡æ¡£ ===")

        all_documents = []
        for file_path, _ in sample_docs:
            loader = TextLoader(file_path, encoding="utf-8")
            docs = loader.load()
            all_documents.extend(docs)

        print(f"âœ“ åŠ è½½äº† {len(all_documents)} ä¸ªæ–‡æ¡£")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, chunk_overlap=50
        )
        splits = text_splitter.split_documents(all_documents)
        print(f"âœ“ æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(splits)} ä¸ªåˆ†å—")

        print("\n=== 3. åˆ›å»ºå‘é‡æ•°æ®åº“ ===")

        api_key = os.getenv("OPENAI_API_KEY", "")
        base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")

        embeddings = OpenAIEmbeddings(api_key=SecretStr(api_key), base_url=base_url)
        vectorstore = Chroma.from_documents(
            documents=splits, embedding=embeddings, persist_directory="./chroma_db"
        )
        print("âœ“ å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆ")

        print("\n=== 4. åˆ›å»º RAG é—®ç­”é“¾ (LCEL) ===")

        model_name = os.getenv("MODEL_NAME", "gpt-3.5-turbo")
        llm = ChatOpenAI(
            model=model_name,
            temperature=0,
            api_key=SecretStr(api_key),
            base_url=base_url,
        )

        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

        prompt = ChatPromptTemplate.from_template(
            """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{input}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ï¼š"""
        )

        def format_docs(docs):
            """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
            return "\n\n".join(doc.page_content for doc in docs)

        # LCEL RAG é“¾
        rag_chain = (
            {"context": retriever | format_docs, "input": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        print("âœ“ RAG é—®ç­”é“¾åˆ›å»ºå®Œæˆ")

        print("\n=== 5. æµ‹è¯• RAG é—®ç­” ===")

        test_questions = [
            "Python æœ‰å“ªäº›ä¸»è¦ç‰¹ç‚¹ï¼Ÿ",
            "LangChain æä¾›å“ªäº›æ ¸å¿ƒåŠŸèƒ½ï¼Ÿ",
            "Python æ˜¯è°åˆ›å»ºçš„ï¼Ÿ",
        ]

        for question in test_questions:
            print(f"\né—®é¢˜ï¼š{question}")
            result = rag_chain.invoke(question)
            print(f"å›ç­”ï¼š{result}")

        print("\n=== 6. æ·»åŠ æ–°æ–‡æ¡£ ===")

        new_content = """
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼ã€‚
æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
å¸¸è§çš„æ·±åº¦å­¦ä¹ æ¡†æ¶åŒ…æ‹¬ TensorFlowã€PyTorchã€Keras ç­‰ã€‚
"""

        with open("temp_docs/deep_learning.txt", "w", encoding="utf-8") as f:
            f.write(new_content.strip())

        new_loader = TextLoader("temp_docs/deep_learning.txt", encoding="utf-8")
        new_docs = new_loader.load()
        new_splits = text_splitter.split_documents(new_docs)
        vectorstore.add_documents(new_splits)

        print(f"âœ“ æ·»åŠ äº† {len(new_splits)} ä¸ªæ–°çš„æ–‡æ¡£åˆ†å—")

        print("\né—®é¢˜ï¼šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")
        result = rag_chain.invoke("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")
        print(f"å›ç­”ï¼š{result}")

        print("\nğŸ‰ RAG QA (LCEL) ç¤ºä¾‹è¿è¡ŒæˆåŠŸï¼")

    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯ï¼š{e}")
        return 1
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯ï¼š{e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
