#!/usr/bin/env python3
"""
11 - ç”Ÿäº§çº§è¿½è¸ª
ä½¿ç”¨ LangSmith è¿›è¡Œè¿½è¸ªã€æ—¥å¿—è®°å½•å’Œæ€§èƒ½ç›‘æ§ï¼Œä½¿ç”¨å…¬å…±æ¨¡å—
"""

import os
import sys
import json
from typing import Dict, Any
from datetime import datetime
from dataclasses import dataclass, asdict
from dotenv import load_dotenv

load_dotenv(override=True)

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils import PerformanceMonitor, CustomCallbackHandler, setup_langsmith


def example_simple_chain_with_tracing(monitor, callback):
    """ç¤ºä¾‹ 1: ç®€å• Chain è¿½è¸ª"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 1: ç®€å• Chain è¿½è¸ª")
    print("="*60)

    try:
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser

        from clients import create_model_client

        llm = create_model_client(temperature=0)
        prompt = ChatPromptTemplate.from_template("å›ç­”ï¼š{question}")

        chain = prompt | llm | StrOutputParser()

        monitor.start_tracking()

        response = chain.invoke(
            {"question": "ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ"},
            config={
                "tags": ["production", "simple"],
                "metadata": {"version": "1.0", "user_id": "demo"}
            }
        )

        metrics = monitor.end_tracking("simple_chain", True)
        print(f"å“åº”: {response}")
        print(f"æ‰§è¡Œæ—¶é—´: {metrics.execution_time:.2f}ç§’")

    except Exception as e:
        metrics = monitor.end_tracking("simple_chain", False, str(e))
        print(f"é”™è¯¯: {e}")


def example_agent_with_tracing(monitor, callback):
    """ç¤ºä¾‹ 2: Agent è¿½è¸ª"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 2: Agent è¿½è¸ª")
    print("="*60)

    monitor.start_tracking()

    try:
        from langchain.tools import tool
        from langchain.agents import AgentExecutor, create_tool_calling_agent
        from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

        from clients import create_model_client

        llm = create_model_client(temperature=0)

        @tool
        def calculator(expression: str) -> str:
            """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
            try:
                result = eval(expression)
                return f"è®¡ç®—ç»“æœ: {result}"
            except:
                return "è®¡ç®—é”™è¯¯"

        tools = [calculator]

        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨è®¡ç®—å™¨å·¥å…·ã€‚"),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])

        agent = create_tool_calling_agent(llm, tools, prompt)
        agent_executor = AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=False,
            max_iterations=3
        )

        response = agent_executor.invoke(
            {"input": "è®¡ç®— 25 * 4 + 18 ç­‰äºå¤šå°‘ï¼Ÿ"},
            config={
                "tags": ["production", "agent"],
                "metadata": {"version": "1.0", "agent_type": "calculator"}
            }
        )

        metrics = monitor.end_tracking("agent_chain", True)
        print(f"å“åº”: {response['output']}")
        print(f"æ‰§è¡Œæ—¶é—´: {metrics.execution_time:.2f}ç§’")

    except Exception as e:
        metrics = monitor.end_tracking("agent_chain", False, str(e))
        print(f"é”™è¯¯: {e}")


def example_rag_with_tracing(monitor, callback):
    """ç¤ºä¾‹ 3: RAG è¿½è¸ª"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 3: RAG è¿½è¸ª")
    print("="*60)

    try:
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        from langchain_community.embeddings import FakeEmbeddings
        from langchain_community.vectorstores import Chroma

        from clients import create_model_client

        llm = create_model_client(temperature=0)

        documents = [
            "LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ã€‚",
            "LangChain æä¾›äº†é“¾å¼è°ƒç”¨ã€æç¤ºè¯ç®¡ç†ç­‰åŠŸèƒ½ã€‚",
            "LangChain æ”¯æŒå¤šç§ LLM æä¾›å•†å’Œå·¥å…·ã€‚",
        ]

        print("âš ï¸  ä½¿ç”¨ FakeEmbeddingsï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰")
        embeddings = FakeEmbeddings(size=1536)
        vectorstore = Chroma.from_texts(documents, embeddings)
        retriever = vectorstore.as_retriever()

        prompt = ChatPromptTemplate.from_template("""
        åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

        ä¸Šä¸‹æ–‡ï¼š{context}

        é—®é¢˜ï¼š{question}

        å›ç­”ï¼š
        """
        )

        chain = (
            {"context": retriever, "question": lambda x: x["question"]}
            | prompt
            | llm
            | StrOutputParser()
        )

        monitor.start_tracking()

        response = chain.invoke(
            {"question": "LangChain æœ‰ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ"},
            config={
                "tags": ["production", "rag"],
                "metadata": {"version": "1.0", "retriever_type": "chroma"}
            }
        )

        metrics = monitor.end_tracking("rag_chain", True)
        print(f"å“åº”: {response}")
        print(f"æ‰§è¡Œæ—¶é—´: {metrics.execution_time:.2f}ç§’")

    except Exception as e:
        metrics = monitor.end_tracking("rag_chain", False, str(e))
        print(f"é”™è¯¯: {e}")


def example_performance_comparison(monitor):
    """ç¤ºä¾‹ 4: æ€§èƒ½å¯¹æ¯”"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 4: æ€§èƒ½å¯¹æ¯”")
    print("="*60)

    try:
        from clients import create_model_client

        test_question = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"

        model_name = os.getenv("MODEL_NAME", "gpt-3.5-turbo")

        print(f"\næµ‹è¯•æ¨¡å‹: {model_name}")

        try:
            llm = create_model_client(temperature=0)

            monitor.start_tracking()

            response = llm.invoke(test_question)

            metrics = monitor.end_tracking(f"model_{model_name}", True)
            print(f"å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
            print(f"æ‰§è¡Œæ—¶é—´: {metrics.execution_time:.2f}ç§’")

        except Exception as e:
            metrics = monitor.end_tracking(f"model_{model_name}", False, str(e))
            print(f"é”™è¯¯: {e}")

    except Exception as e:
        print(f"æ€§èƒ½å¯¹æ¯”é”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¦œğŸ”— 11 - ç”Ÿäº§çº§è¿½è¸ª")
    print("=" * 60)

    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return 1

    langsmith_enabled = setup_langsmith()

    monitor = PerformanceMonitor()
    callback = CustomCallbackHandler()

    try:
        example_simple_chain_with_tracing(monitor, callback)
        example_agent_with_tracing(monitor, callback)
        example_rag_with_tracing(monitor, callback)
        example_performance_comparison(monitor)

        print("\n" + "="*60)
        print("æ€§èƒ½æ‘˜è¦")
        print("="*60)

        summary = monitor.get_summary()
        print(json.dumps(summary, indent=2, ensure_ascii=False))

        monitor.save_metrics()
        callback.save_logs()

        if langsmith_enabled:
            print("\nâœ“ è®¿é—® LangSmith æŸ¥çœ‹è¯¦ç»†è¿½è¸ª:")
            print("  https://smith.langchain.com/")

        print("\nğŸ‰ ç”Ÿäº§çº§è¿½è¸ªç¤ºä¾‹è¿è¡Œå®Œæˆï¼")

    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
