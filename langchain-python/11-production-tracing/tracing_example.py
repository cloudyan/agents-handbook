#!/usr/bin/env python3
"""
11 - ç”Ÿäº§çº§è¿½è¸ª
ä½¿ç”¨ LangSmith è¿›è¡Œè¿½è¸ªã€æ—¥å¿—è®°å½•å’Œæ€§èƒ½ç›‘æ§
"""

import os
import time
import json
from typing import Dict, Any
from datetime import datetime
from dataclasses import dataclass, asdict
from dotenv import load_dotenv
from pydantic import SecretStr

load_dotenv(override=True)


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    chain_name: str
    execution_time: float
    input_tokens: int
    output_tokens: int
    total_tokens: int
    success: bool
    error_message: str = ""


class ProductionMonitor:
    """ç”Ÿäº§ç¯å¢ƒç›‘æ§å™¨"""

    def __init__(self):
        self.metrics_history = []
        self.start_time = None

    def start_tracking(self):
        """å¼€å§‹è¿½è¸ª"""
        self.start_time = time.time()

    def end_tracking(self, chain_name: str, success: bool, error: str = "") -> PerformanceMetrics:
        """ç»“æŸè¿½è¸ªå¹¶è®°å½•æŒ‡æ ‡"""
        if not self.start_time:
            raise ValueError("å¿…é¡»å…ˆè°ƒç”¨ start_tracking()")

        execution_time = time.time() - self.start_time

        metrics = PerformanceMetrics(
            chain_name=chain_name,
            execution_time=execution_time,
            input_tokens=0,
            output_tokens=0,
            total_tokens=0,
            success=success,
            error_message=error
        )

        self.metrics_history.append(metrics)
        self.start_time = None

        return metrics

    def get_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if not self.metrics_history:
            return {"message": "æ²¡æœ‰è®°å½•çš„æŒ‡æ ‡"}

        total_runs = len(self.metrics_history)
        successful_runs = sum(1 for m in self.metrics_history if m.success)
        failed_runs = total_runs - successful_runs

        avg_time = sum(m.execution_time for m in self.metrics_history) / total_runs
        total_tokens = sum(m.total_tokens for m in self.metrics_history)

        return {
            "total_runs": total_runs,
            "successful_runs": successful_runs,
            "failed_runs": failed_runs,
            "success_rate": successful_runs / total_runs if total_runs > 0 else 0,
            "average_time": avg_time,
            "total_tokens": total_tokens,
            "estimated_cost": total_tokens * 0.00002,
        }

    def save_metrics(self, filename: str = "performance_metrics.json"):
        """ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        data = {
            "timestamp": datetime.now().isoformat(),
            "summary": self.get_summary(),
            "metrics": [asdict(m) for m in self.metrics_history]
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"âœ“ æŒ‡æ ‡å·²ä¿å­˜åˆ° {filename}")


class CustomCallbackHandler:
    """è‡ªå®šä¹‰å›è°ƒå¤„ç†å™¨"""

    def __init__(self):
        self.logs = []

    def on_llm_start(self, serialized, prompts, **kwargs):
        """LLM è°ƒç”¨å¼€å§‹"""
        self.log("INFO", f"LLM è°ƒç”¨å¼€å§‹: {prompts[0][:50]}...")

    def on_llm_end(self, response, **kwargs):
        """LLM è°ƒç”¨ç»“æŸ"""
        self.log("INFO", "LLM è°ƒç”¨å®Œæˆ")

    def on_llm_error(self, error, **kwargs):
        """LLM è°ƒç”¨é”™è¯¯"""
        self.log("ERROR", f"LLM é”™è¯¯: {error}")

    def on_chain_start(self, serialized, inputs, **kwargs):
        """Chain è°ƒç”¨å¼€å§‹"""
        chain_name = serialized.get("name", "unknown")
        self.log("INFO", f"Chain '{chain_name}' å¼€å§‹æ‰§è¡Œ")

    def on_chain_end(self, outputs, **kwargs):
        """Chain è°ƒç”¨ç»“æŸ"""
        self.log("INFO", "Chain æ‰§è¡Œå®Œæˆ")

    def on_chain_error(self, error, **kwargs):
        """Chain è°ƒç”¨é”™è¯¯"""
        self.log("ERROR", f"Chain é”™è¯¯: {error}")

    def on_tool_start(self, serialized, input_str, **kwargs):
        """Tool è°ƒç”¨å¼€å§‹"""
        tool_name = serialized.get("name", "unknown")
        self.log("INFO", f"Tool '{tool_name}' å¼€å§‹æ‰§è¡Œ: {input_str[:30]}...")

    def on_tool_end(self, output, **kwargs):
        """Tool è°ƒç”¨ç»“æŸ"""
        self.log("INFO", f"Tool æ‰§è¡Œå®Œæˆ: {output[:50]}...")

    def on_tool_error(self, error, **kwargs):
        """Tool è°ƒç”¨é”™è¯¯"""
        self.log("ERROR", f"Tool é”™è¯¯: {error}")

    def log(self, level: str, message: str):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] {message}"
        self.logs.append(log_entry)
        print(log_entry)

    def save_logs(self, filename: str = "reports/execution_logs.txt"):
        """ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶"""
        with open(filename, "w", encoding="utf-8") as f:
            f.write("\n".join(self.logs))

        print(f"âœ“ æ—¥å¿—å·²ä¿å­˜åˆ° {filename}")


def setup_langsmith():
    """é…ç½® LangSmith è¿½è¸ª"""
    if not os.getenv("LANGSMITH_API_KEY"):
        print("âš ï¸  æœªè®¾ç½® LANGSMITH_API_KEYï¼ŒLangSmith è¿½è¸ªå·²ç¦ç”¨")
        print("   è®¿é—® https://smith.langchain.com/ è·å– API Key")
        return False

    project_name = os.getenv("LANGSMITH_PROJECT", "agents-handbook")
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGSMITH_PROJECT"] = project_name

    print("âœ“ LangSmith è¿½è¸ªå·²å¯ç”¨")
    print(f"  é¡¹ç›®åç§°: {project_name}")
    print(f"  è¿½è¸ªåœ°å€: https://smith.langchain.com/")
    return True


def example_simple_chain_with_tracing(monitor, callback, openai_api_key, openai_base_url, model_name):
    """ç¤ºä¾‹ 1: ç®€å• Chain è¿½è¸ª"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 1: ç®€å• Chain è¿½è¸ª")
    print("="*60)

    try:
        from langchain_openai import ChatOpenAI
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser

        llm = ChatOpenAI(
            model=model_name,
            temperature=0,
            api_key=SecretStr(openai_api_key),
            base_url=openai_base_url
        )
        prompt = ChatPromptTemplate.from_template("å›ç­”ï¼š{question}")

        chain = prompt | llm | StrOutputParser()

        monitor.start_tracking()

        response = chain.invoke(
            {"question": "ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ"},
            config={
                "tags": ["production", "simple"],
                "metadata": {"version": "1.0", "user_id": "demo"}
            }
        )

        metrics = monitor.end_tracking("simple_chain", True)
        print(f"å“åº”: {response}")
        print(f"æ‰§è¡Œæ—¶é—´: {metrics.execution_time:.2f}ç§’")

    except Exception as e:
        metrics = monitor.end_tracking("simple_chain", False, str(e))
        print(f"é”™è¯¯: {e}")


def example_agent_with_tracing(monitor, callback, openai_api_key, openai_base_url, model_name):
    """ç¤ºä¾‹ 2: Agent è¿½è¸ª"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 2: Agent è¿½è¸ª")
    print("="*60)

    try:
        from langchain_openai import ChatOpenAI
        from langchain.agents import tool, AgentExecutor, create_tool_calling_agent
        from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

        llm = ChatOpenAI(
            model=model_name,
            temperature=0,
            api_key=SecretStr(openai_api_key),
            base_url=openai_base_url
        )

        @tool
        def calculator(expression: str) -> str:
            """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
            try:
                result = eval(expression)
                return f"è®¡ç®—ç»“æœ: {result}"
            except:
                return "è®¡ç®—é”™è¯¯"

        tools = [calculator]

        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨è®¡ç®—å™¨å·¥å…·ã€‚"),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])

        agent = create_tool_calling_agent(llm, tools, prompt)
        agent_executor = AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=True,
            max_iterations=3
        )

        monitor.start_tracking()

        response = agent_executor.invoke(
            {"input": "è®¡ç®— 25 * 4 + 18 ç­‰äºå¤šå°‘ï¼Ÿ"},
            config={
                "tags": ["production", "agent"],
                "metadata": {"version": "1.0", "agent_type": "calculator"}
            }
        )

        metrics = monitor.end_tracking("agent_chain", True)
        print(f"å“åº”: {response['output']}")
        print(f"æ‰§è¡Œæ—¶é—´: {metrics.execution_time:.2f}ç§’")

    except Exception as e:
        metrics = monitor.end_tracking("agent_chain", False, str(e))
        print(f"é”™è¯¯: {e}")


def example_rag_with_tracing(monitor, callback, openai_api_key, openai_base_url, model_name):
    """ç¤ºä¾‹ 3: RAG è¿½è¸ª"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 3: RAG è¿½è¸ª")
    print("="*60)

    try:
        from langchain_openai import ChatOpenAI
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        from langchain_community.embeddings import FakeEmbeddings
        from langchain_community.vectorstores import Chroma

        llm = ChatOpenAI(
            model=model_name,
            temperature=0,
            api_key=SecretStr(openai_api_key),
            base_url=openai_base_url
        )

        documents = [
            "LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ã€‚",
            "LangChain æä¾›äº†é“¾å¼è°ƒç”¨ã€æç¤ºè¯ç®¡ç†ç­‰åŠŸèƒ½ã€‚",
            "LangChain æ”¯æŒå¤šç§ LLM æä¾›å•†å’Œå·¥å…·ã€‚",
        ]

        embeddings = FakeEmbeddings(size=1536)
        vectorstore = Chroma.from_texts(documents, embeddings)
        retriever = vectorstore.as_retriever()

        prompt = ChatPromptTemplate.from_template("""
        åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

        ä¸Šä¸‹æ–‡ï¼š{context}

        é—®é¢˜ï¼š{question}

        å›ç­”ï¼š
        """)

        chain = (
            {"context": retriever, "question": lambda x: x["question"]}
            | prompt
            | llm
            | StrOutputParser()
        )

        monitor.start_tracking()

        response = chain.invoke(
            {"question": "LangChain æœ‰ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ"},
            config={
                "tags": ["production", "rag"],
                "metadata": {"version": "1.0", "retriever_type": "chroma"}
            }
        )

        metrics = monitor.end_tracking("rag_chain", True)
        print(f"å“åº”: {response}")
        print(f"æ‰§è¡Œæ—¶é—´: {metrics.execution_time:.2f}ç§’")

    except Exception as e:
        metrics = monitor.end_tracking("rag_chain", False, str(e))
        print(f"é”™è¯¯: {e}")


def example_performance_comparison(monitor, openai_api_key, openai_base_url, model_name):
    """ç¤ºä¾‹ 4: æ€§èƒ½å¯¹æ¯”"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹ 4: æ€§èƒ½å¯¹æ¯”")
    print("="*60)

    try:
        from langchain_openai import ChatOpenAI

        test_question = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"

        models = [model_name]

        for model in models:
            print(f"\næµ‹è¯•æ¨¡å‹: {model}")

            try:
                llm = ChatOpenAI(
                    model=model,
                    temperature=0,
                    api_key=SecretStr(openai_api_key),
                    base_url=openai_base_url
                )

                monitor.start_tracking()

                response = llm.invoke(test_question)

                metrics = monitor.end_tracking(f"model_{model}", True)
                print(f"å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
                print(f"æ‰§è¡Œæ—¶é—´: {metrics.execution_time:.2f}ç§’")

            except Exception as e:
                metrics = monitor.end_tracking(f"model_{model}", False, str(e))
                print(f"é”™è¯¯: {e}")

    except Exception as e:
        print(f"æ€§èƒ½å¯¹æ¯”é”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¦œğŸ”— 11 - ç”Ÿäº§çº§è¿½è¸ª")
    print("=" * 60)

    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    openai_api_key = os.getenv("OPENAI_API_KEY", "")
    openai_base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    model_name = os.getenv("MODEL_NAME", "gpt-3.5-turbo")

    if not openai_api_key:
        print("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return 1

    langsmith_enabled = setup_langsmith()

    monitor = ProductionMonitor()
    callback = CustomCallbackHandler()

    try:
        example_simple_chain_with_tracing(monitor, callback, openai_api_key, openai_base_url, model_name)
        example_agent_with_tracing(monitor, callback, openai_api_key, openai_base_url, model_name)
        example_rag_with_tracing(monitor, callback, openai_api_key, openai_base_url, model_name)
        example_performance_comparison(monitor, openai_api_key, openai_base_url, model_name)

        print("\n" + "="*60)
        print("æ€§èƒ½æ‘˜è¦")
        print("="*60)

        summary = monitor.get_summary()
        print(json.dumps(summary, indent=2, ensure_ascii=False))

        monitor.save_metrics()
        callback.save_logs()

        if langsmith_enabled:
            print("\nâœ“ è®¿é—® LangSmith æŸ¥çœ‹è¯¦ç»†è¿½è¸ª:")
            print("  https://smith.langchain.com/")

        print("\nğŸ‰ ç”Ÿäº§çº§è¿½è¸ªç¤ºä¾‹è¿è¡Œå®Œæˆï¼")

    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
