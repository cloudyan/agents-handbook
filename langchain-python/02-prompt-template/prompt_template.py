#!/usr/bin/env python3
"""
02 - Prompt Template (LCEL ç‰ˆæœ¬)
æ·±å…¥å­¦ä¹  LangChain çš„æç¤ºè¯æ¨¡æ¿åŠŸèƒ½
"""

import os
from dotenv import load_dotenv
from pydantic import SecretStr

load_dotenv(override=True)


def main():
    print("ğŸ¦œğŸ”— 02 - Prompt Template (LCEL)")
    print("=" * 40)

    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return 1

    try:
        from langchain_openai import ChatOpenAI
        from langchain_core.prompts import (
            ChatPromptTemplate,
            SystemMessagePromptTemplate,
            HumanMessagePromptTemplate,
        )
        from langchain_core.output_parsers import StrOutputParser

        print("âœ“ LangChain ç»„ä»¶å¯¼å…¥å®Œæˆ")

        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        api_key = os.getenv("OPENAI_API_KEY", "")
        base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        model_name = os.getenv("MODEL_NAME", "gpt-3.5-turbo")

        # åˆå§‹åŒ–æ¨¡å‹
        llm = ChatOpenAI(
            model=model_name,
            temperature=0.7,
            api_key=SecretStr(api_key),
            base_url=base_url,
        )
        print("âœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        # 1. åŸºç¡€æç¤ºè¯æ¨¡æ¿
        print("\n=== 1. åŸºç¡€æ¨¡æ¿ ===")
        simple_template = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ª{role}ã€‚
è¯·å›ç­”ï¼š{question}
""")

        chain = simple_template | llm | StrOutputParser()

        response = chain.invoke({
            "role": "Python ç¨‹åºå‘˜",
            "question": "Python çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
        })
        print(f"å›ç­”ï¼š{response}")

        print("\n=== 2. ç»“æ„åŒ–æ¨¡æ¿ + LCEL ===")
        system_template = SystemMessagePromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{role}ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ï¼š{task}
è¯·ç”¨{language}å›ç­”ï¼Œä¿æŒ{tone}çš„è¯­è°ƒã€‚
""")

        human_template = HumanMessagePromptTemplate.from_template("""
ç”¨æˆ·é—®é¢˜ï¼š{question}
""")

        chat_template = ChatPromptTemplate.from_messages(
            [system_template, human_template]
        )

        chain = chat_template | llm | StrOutputParser()

        response = chain.invoke({
            "role": "æ•°æ®ç§‘å­¦å®¶",
            "task": "è§£é‡Šæœºå™¨å­¦ä¹ æ¦‚å¿µ",
            "language": "ä¸­æ–‡",
            "tone": "ä¸“ä¸šä¸”æ˜“æ‡‚",
            "question": "ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿ",
        })
        print(f"å›ç­”ï¼š{response}")

        print("\n=== 3. å¤šè§’è‰²å¯¹æ¯” + LCEL ===")
        roles = [
            {"role": "å¹¼å„¿å›­è€å¸ˆ", "tone": "è€å¿ƒæ¸©æŸ”", "language": "ç®€å•çš„ä¸­æ–‡"},
            {"role": "å¤§å­¦æ•™æˆ", "tone": "å­¦æœ¯ä¸¥è°¨", "language": "ä¸“ä¸šçš„ä¸­æ–‡"},
        ]

        question = "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ"

        for role_info in roles:
            print(f"\n--- {role_info['role']}çš„å›ç­” ---")
            response = chain.invoke({
                "role": role_info["role"],
                "task": "è§£é‡Šè‡ªç„¶ç°è±¡",
                "language": role_info["language"],
                "tone": role_info["tone"],
                "question": question,
            })
            print(response)

        print("\nğŸ‰ Prompt Template (LCEL) ç¤ºä¾‹è¿è¡ŒæˆåŠŸï¼")

    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯ï¼š{e}")
        return 1
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯ï¼š{e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
